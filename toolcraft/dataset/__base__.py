"""
NOTE: dont be tempted to use `channels_first` field in PreparedData (read below)
  Note that this field is not mandated in PreparedData as we want to save
  disk space by generating data for both channel first and channel last
  ... but FosteredData and Dataset will be extracted from PreparedData and
  are not designed for long term storage ... they will be created on
  SSD's or system memory for temporary use and fast access ...
  Also remember Dataset do not use any storage they are constructed on top
  of FosteredData
"""


import abc
import dataclasses
import typing as t
import pathlib
import tensorflow as tf
import pyarrow as pa
from tensorflow import keras as tk
import numpy as np

from .. import util, logger, gui, settings
from .. import marshalling as m
from .. import error as e
from .. import storage as s
from .. import algo

NPY_OR_TENSOR = t.Union[np.ndarray, tf.Tensor]

_LOGGER = logger.get_logger()


@dataclasses.dataclass(frozen=True)
class PreparedFileGroup(s.NpyFileGroup, abc.ABC):
    """
    PreparedFile are always NpyRecordFile.

    What we do here?
    + download files,
    + then create some temporary files if needed and
    + finally prepare prepared files which are npy_structs

    Why to use with Dataset class?
    + Make PreparedFile as properties and based on fields in Dataset class
      access ProviderFiles to create Dataset
    + Note that for some dataset we might need more than one PreparedFiles

    While the Dataset and PreparedFile are responsible to get foster
    data based on setting provided by user. What we mean by foster data is
    that prepared files are complete data while fostered files are based on
    user settings for an experiment. This will allow below advantages:
    + local storage for foster data when provided data is on network disk
    + No need to apply the transformation on data if foster data found locally
      - i.e. any filtering or transformations you do once will stay cached
    + No need to load entire provider data
    + Provider can use multiple PreparedFiles to create single dataset out of it
    + todo: In future we can slice foster data like 200 mb chunks on disk and
        stitch them at runtime using tf.data.Dataset API (for fast access)
    + todo: can implement parallel io
    + todo: can duplicate data redundantly for faster access
    + todo: can keep provider data on HDD and foster data on SSD ;)

    """

    @property
    def store_fields_location(self) -> pathlib.Path:
        return settings.Dir.ANALYSIS / "Pr" / self.name

    @property
    @abc.abstractmethod
    def name(self) -> str:
        """
        We make this property abstract here so that all child classes
        implement it to return readable unique name per instance. This
        override behaviour of parent class which was returning hex_hash.

        Reason:
          There are limited number of prepared data sets. And we will keep
          them in permanent storage. So why not have convenient readable name ;)

        """
        ...

    @property
    def title(self) -> str:
        return self.name

    @property
    @util.CacheResult
    def path(self) -> pathlib.Path:
        return settings.Dir.PREPARED / self.group_by_name / self.name

    @property
    def group_by_name(self) -> str:
        """
        We assume that prepared data will mostly have readable unique names
        and they will be few per module ... so we have group by name using
        module name
        """
        return self.__module__

    # noinspection PyTypeChecker
    @classmethod
    def generate(cls) -> t.List["PreparedFileGroup"]:
        """
        A method that can generate all prepared data on users disk
        automatically.

        If this class has some fields then override this method to generate
        data for all possible combinations that depends on those dataclass
        fields.
        """
        raise Exception(
            f"This method is not supported for class {cls}",
            f"If you want to use it please override it ..."
        )

    @classmethod
    def kaggle_create(cls):
        """

        For metadata
        https://github.com/Kaggle/kaggle-api/wiki/Dataset-Metadata

        check rough_work/try_kaggle.py
        """
        # todo: this can be achieved ... we can created dataset files and do
        #  some magic to sync or backup public datasets ... generated by us
        # todo: if already present check hash from info file and raise error
        #  if does not match ... if not present upload it :)
        # todo: if public dataset created by us then cls..create() can check
        #  if created files on kaggle and then download or else create and
        #  upload it :)
        ...
        # import kaggle
        # kaggle.KaggleApi
        # kaggle.ApiClient

    def plot_anomalous_examples(self):
        """
        todo implement this with keras autoencoders ... RESEARCH
        https://www.pyimagesearch.com/2020/03/02/
        anomaly-detection-with-keras-tensorflow-and-deep-learning/
        """
        raise Exception(
            f"This method is not supported for class {self.__class__}",
            f"If you want to use it please override it ..."
        )


class FosteredFileGroupInternal(m.Internal):
    dataset: "Dataset"


@dataclasses.dataclass(frozen=True)
class FosteredFileGroup(s.NpyFileGroup, abc.ABC):
    """
    Completely dictated by Dataset class ...
    Currently, similar to PreparedFile FosteredFile is NpyFile
    todo: look for other formats and support for chunked files

    ProvidedFile is whole dataset and FosteredFile is derived from
    ProvidedFile. The deriving steps are like slicing, augmenting etc.

    Note the trigger chain:
    + FosteredFile.get triggers FosteredFile.create
    + FosteredFile.create triggers PreparedFile.get
    + PreparedFile.create triggers DownloadFile.get (or TempFile.get)
    + DownloadFile.get (or TempFile.get) triggers
        DownloadFile.create (or DownloadFile.get -> and hence
        DownloadFile.create)

    Rationale for trigger chain:
     + on a remote machine for a given Dataset based on dataset fields a
       fostered file is created and only what is required is downloaded/created
       on that machine
     + leads to disk storage efficiency and less transfer on wire
     + todo: option to store date on fast network storage, HDD or SSD

    todo: we can have rest data servers for PreparedFile so that
      FosteredFile can access streamed data over network ... (advanced thing
      to implement) ... refer apache arrow stream dataset and in memory
      dataset ... maybe we can stream prepared data from data and servers
      and fostered data will be saved in memory bypassing usage of disk and
      when code exits the system-memory will be freed .... thus we can
      achieve disk-less compute servers/nodes

    todo: ui components with some interactions and status broadcast for:
      + dataset_core.DownloadFile
      + dataset_core.TempFile
      + dataset_core.PreparedFile
      + dataset_core.FosteredFile

    todo: as there can be piles of files being created in
      config.FOSTER_DATA_DIR directory we can check for last_accessed_on in
      *.config file and gracefully delete files periodically i.e. we can
      implement some auto delete mechanism when the program starts or exists
      and if periodic_check_needed ... maybe we can have some dunder method
      __delete__ and delete files based on last_accessed_on. I assume the code
      will be same for TempFile and FosteredFile

    """
    class LITERAL(s.NpyFileGroup.LITERAL):
        dataset_key = "__dataset__"

    # the prepared data from which we foster data
    prepared_data: PreparedFileGroup

    @property
    def group_by_name(self) -> str:
        return self.prepared_data.name

    @property
    @abc.abstractmethod
    def num_examples(self) -> int:
        ...

    @property
    @util.CacheResult
    def internal(self) -> FosteredFileGroupInternal:
        return FosteredFileGroupInternal(owner=self)

    @property
    @util.CacheResult
    def path(self) -> pathlib.Path:
        return settings.Dir.FOSTERED / self.group_by_name / self.name

    # noinspection PyTypeChecker
    @property
    def is_outdated(self) -> bool:
        """
        Whenever prepared data used to create foster data is deleted then the
        creation time stamp for fostered data is before prepared data and
        hence we want to create fostered data from newly created prepared data
        """
        return self.config.created_on < self.prepared_data.config.created_on

    @property
    def is_auto_hash(self) -> bool:
        return True

    @property
    @abc.abstractmethod
    def is_deterministic(self) -> bool:
        """
        The property which informs if the foster data will generate
        deterministic batches on every run
        """
        ...

    @property
    def load_data_chunk_size(self) -> int:
        """
        Controls the amount of data that will be fetched from teh disk with
        NpyMemMaps

        todo: work on some benchmarks so that we can adaptively find best
          chunk_size so that data is loaded efficiently ... note that foster
          data slices the underlying numpy array so for every instance of
          Foster data we might need different chunk size ... we need to
          research this ... and save the chunk_size in config file so that
          future runs need not estimate best chunk size ;)
          Right now it is constant
        """
        return 1024

    @property
    @util.CacheResult
    def tensor_spec(self) -> t.Dict[str, tf.TensorSpec]:
        """
        How will the mini batch look like the shape and type ... useful when
        building models for most optimal performance ...
        Apart from that the dataset need not be iterated to get the
        tensor_spec ... which makes this super useful to build optimized
        models without knowing shape of tensors at runtime ...
        Only drawback is every subclass needs to supply the tensor spec
        without loading the data i.e. shape and type of tensors need to be
        known before hand ...

        We will estimate this by using
        + batch_size from dataset property
        + shape and dtype of NpyMemMaps
        + also based on batch_transform method

        Generic spec is derived from batch_size and the files in this FileGroup
        """
        # check if arbitrary lengths
        if self.has_arbitrary_lengths:
            e.code.NotSupported(
                msgs=[
                    f"The files in this file group {self.__class__} "
                    f"have arbitrary lengths so "
                    f"generic batch_tensor_spec is not suitable",
                    f"Please override or refrain from using this property."
                ]
            )

        # return container
        _ret = {}
        for k, v in self.all_npy_mem_maps_cache.items():
            _ret[k] = tf.TensorSpec(
                # note that we need to repeat singular elements so we fix
                # first dimension for singular elements ... i.e. those items
                # will be repeated in a chunk as well as across all chunks ...
                # so please take care to repeat them in generator
                shape=util.add_channels_info_to_shape_tuple(
                    shape=(None, ) + v.shape[1:],
                    num_channels=1,
                    channel_first=settings.Tf.CHANNELS_FIRST,
                ),
                dtype=v.dtype,
                name=k,
            )

        # return
        return _ret

    @property
    @util.CacheResult
    def container(self) -> t.Dict[str, np.ndarray]:
        """
        This property makes a container that will be used by generator to make
        the batches .... this reuses the memory and the `generator()` method
        overrides the values in container everytime you yield.

        Note that we do not cache it as the result for this will be a local
        variable inside generator and hence consecutive yields will anyways
        reuse memory

        The generic implementation will generate based on batch_size while
        singular elements will be save as it is
        """

        # container dict
        # todo: we might be able to save dataset over this dict if we have
        #  custom OurDict class that extend builtin dict ... this will make
        #  self.dataset available to yielded dict in for loop ... TEST IF IT
        #  WORKS ... FOR NOW POSTPONING EFFORTS ... also low priority as this
        #  is hacky but anyways there no other way to achieve the same
        _ret = {}

        # loop over batch_tensor_spec
        for k, v in self.tensor_spec.items():
            _ret[k] = np.zeros(
                shape=[self.load_data_chunk_size] + v.shape.as_list()[1:],
                dtype=v.dtype.as_numpy_dtype
            )

        # return
        return _ret

    def generator(self) -> t.Iterable[t.Dict[str, np.ndarray]]:
        """
        This is a generic generator any specific generator needs to be
        overridden.

        Why use shuffle_seed='NO_SHUFFLE'
          This helps fast access. While note that while creating foster data
          from which this dict is loaded we shuffled and saved files.
          So ideally no need not read and shuffle again. Check for example
          create_file of FosteredFileGroup and some of its overridden methods.

        The behaviour
        + create chunks based on chunk_size while return singular
          elements as it is
        + Fostered data on disk is read with NO_SHUFFLE
        + batch_container is created only once and we overwrite it everytime
          for each batch ... this helps save creating new memory again and again
        """

        # ------------------------------------------------------------- 01
        # access some vars
        _dataset = self.internal.dataset
        _num_examples = self.num_examples
        _chunk_size = self.load_data_chunk_size
        _num_chunks_to_process = _num_examples // _chunk_size
        _remainder_examples = _num_examples % _chunk_size
        _channel_first = settings.Tf.CHANNELS_FIRST
        # make sure that dataset was called
        if not _dataset.is_called:
            e.code.CodingError(
                msgs=[
                    f"Looks like dataset which uses this foster data was not "
                    f"called with __call__ ..."
                ]
            )
        # check if dataset has opened foster data with NO_SHUFFLE shuffle seed
        if self.internal.on_call_kwargs['shuffle_seed'] != s.NO_SHUFFLE:
            e.code.CodingError(
                msgs=[
                    f"The default generator expects `shuffle_seed=s.NO_SHUFFLE`"
                ]
            )

        # ------------------------------------------------------------- 02
        # get batch container
        _batch_container = self.container

        # ------------------------------------------------------------- 03
        # yield batches
        # ------------------------------------------------------------- 03.01
        # assign all single valued elements while keep non singular
        # NpyMemMaps in dict to be overwritten later ... this helps
        # rewrite per yield while extracting chunks
        _multi_valued_npy_memmaps = {}
        for k, v in _batch_container.items():
            v = self.all_npy_mem_maps_cache[k]
            if len(v) == 1:
                if _channel_first:
                    _batch_container[k][0, :] = v[...]
                else:
                    _batch_container[k][:, 0] = v[...]
            else:
                _multi_valued_npy_memmaps[k] = v
        # ------------------------------------------------------------- 03.02
        # yield examples until continue_looping ...
        for _chunk in range(_num_chunks_to_process):
            _slice_start_at = _chunk * _chunk_size
            _slice = slice(_slice_start_at, _slice_start_at + _chunk_size)
            # extract and assign
            # todo: optimize looping over items if possible i.e. get rid
            #  of this for loop over dict items
            if _channel_first:
                for k, v in _multi_valued_npy_memmaps.items():
                    _batch_container[k][:, 0, ...] = v[_slice]
            else:
                for k, v in _multi_valued_npy_memmaps.items():
                    _batch_container[k][:, ..., 0] = v[_slice]
            # example extracted from examples in above code is now
            # returned as tuple
            # noinspection PyArgumentList
            yield _batch_container
        # ------------------------------------------------------------- 03.03
        # if there are remainder examples then we need to yield one more chunk
        if _remainder_examples > 0:
            # here we overwrite remainder examples ... note similar to above
            # code no need to handle single valued items
            _slice = slice(_num_chunks_to_process * _chunk_size, None)
            if _channel_first:
                for k, v in _multi_valued_npy_memmaps.items():
                    _batch_container[k][:_remainder_examples, 0, ...] = \
                        v[_slice]
            else:
                for k, v in _multi_valued_npy_memmaps.items():
                    _batch_container[k][:_remainder_examples, ..., 0] = \
                        v[_slice]
            # here we yield the remainder examples in final chunk ... so the
            # final chunk will be smaller
            yield {
                k: v[:_remainder_examples]
                for k, v in _batch_container.items()
            }


@dataclasses.dataclass(frozen=True)
class Dataset(m.HashableClass, abc.ABC):
    """
    todo: if you like performance and prefer static batches then go through
      these papers until them do not use static batch sizes
      http://proceedings.mlr.press/v97/agarwal19a/agarwal19a.pdf
      Static Automatic Batching in TensorFlow

    General guideline:
    + Have FosteredFileGroup as property in Dataset class, and Dataset is
      anyways a field for FosteredFileGroup
    + Have PreparedFileGroup as property for FosteredFileGroup ...
    + Have DownloadFileGroup as property for PreparedFileGroup ...
    """
    prepared_data: PreparedFileGroup

    class LITERAL(m.HashableClass.LITERAL):

        # points for embeddings
        points_for_embeddings = 5000

    @property
    @abc.abstractmethod
    def title(self) -> str:
        ...

    @property
    def store_fields_location(self) -> pathlib.Path:
        return settings.Dir.ANALYSIS / "Dataset" / self.group_by_name / \
               self.name

    @property
    @util.CacheResult
    def num_examples(self) -> int:
        return self.foster_data.num_examples

    @property
    def is_deterministic(self) -> bool:
        return self.foster_data.is_deterministic

    @property
    @abc.abstractmethod
    def num_classes(self) -> int:
        """
        Provide when fit_dataset is used for classification
        """
        ...

    @property
    def random_guessing_accuracy(self) -> float:
        return 1 / self.num_classes

    @property
    @abc.abstractmethod
    def label_formatter_for_plotting(self) -> str:
        ...

    @property
    @abc.abstractmethod
    def foster_data(self) -> FosteredFileGroup:
        """
        The FosteredFileGroup that will generate batches from disk
        i.e. using NpyMemMap's.
        """
        ...

    @property
    def base_tensor_spec(self) -> t.Dict[str, tf.TensorSpec]:
        return self.foster_data.tensor_spec

    def __call__(
        self, *,
        as_numpy: bool = False,
        normalize: bool = True,
        batch_size: int = 1,
        **kwargs,
    ) -> "Dataset":

        # ----------------------------------------------------- 02
        # noinspection PyTypeChecker
        return super().__call__(
            as_numpy=as_numpy,
            normalize=normalize,
            batch_size=batch_size,
            **kwargs,
        )

    def on_call(self):

        # ----------------------------------------------------- 01
        # call super
        super().on_call()

        # ----------------------------------------------------- 02
        # get kwargs passed during __call__
        as_numpy: bool = self.internal.on_call_kwargs['as_numpy']
        normalize: bool = self.internal.on_call_kwargs['normalize']
        batch_size: int = self.internal.on_call_kwargs['batch_size']

        # ----------------------------------------------------- 03
        # if normalize check if you have the batch transform method for same
        if normalize:
            if Dataset.batch_transform_normalize == \
                    self.__class__.batch_transform_normalize:
                e.validation.NotAllowed(
                    msgs=[
                        f"You cannot use normalize=True as "
                        f"the class {self.__class__} does not override method "
                        f"{Dataset.batch_transform_normalize}"
                    ]
                )

        # ----------------------------------------------------- 04
        # todo: later ... if we decide static batch_size as of now we are not
        #  doing it
        # if batch_size not in multiple of num_examples check if foster_data
        # can pad random traces in dataset .... also make sure that less than
        # 5% examples are allowed to be padded

        # ----------------------------------------------------- 05
        # context management call for foster_data i.e. decide how to open up
        # foster data
        self.on_call_open_foster_data()

    @abc.abstractmethod
    def on_call_open_foster_data(self):
        """
        Decide on how the foster data will be opened when dataset is called
        """
        ...

    def on_enter(self):
        # ----------------------------------------------------- 01
        # call super
        super().on_enter()
        # ----------------------------------------------------- 02
        # manage the context of foster data
        self.foster_data.on_enter()

    def on_exit(self):
        # ----------------------------------------------------- 01
        # call super
        super().on_exit()
        # ----------------------------------------------------- 02
        # manage the context of foster data
        self.foster_data.on_exit()

    def on_iter(self) -> tf.data.Dataset:
        # ----------------------------------------------------- 01
        # call super
        super().on_iter()

        # ----------------------------------------------------- 02
        # return
        return self.get_tf_dataset()

    def base_generator(self) -> t.Callable:
        return self.foster_data.generator

    def get_keras_inputs(self) -> t.Dict[str, tf.Tensor]:
        _ret = {}
        for _input_field, _specs in self.foster_data.tensor_spec.items():
            e.code.AssertError(
                value1=_specs.shape[0] is None, value2=True,
                msgs=["was expecting unknown batch size"]
            )
            _ret[_input_field] = tk.Input(
                shape=_specs.shape[1:],
                batch_size=_specs.shape[0],
                dtype=_specs.dtype,
                name=f"input.{_input_field}",
            )
        return _ret

    def get_tf_dataset(self) -> tf.data.Dataset:
        """
        todo: explore caching, prefetch etc. here for efficiency
        """
        # test if dataset.__call__ was called
        _on_call_kwargs = self.internal.on_call_kwargs
        if self.internal.on_call_kwargs is None:
            e.code.CodingError(
                msgs=[
                    f"this method can only be used when you have called self "
                    f"so that on_call_kwargs are set. Plus this need to "
                    f"either used within with context or for loop."
                ]
            )

        # get necessary on_call_kwargs
        as_numpy = _on_call_kwargs['as_numpy']
        normalize = _on_call_kwargs['normalize']
        batch_size = _on_call_kwargs['batch_size']

        # get dataset made from generator
        _tf_dataset = tf.data.Dataset.from_generator(
            generator=self.base_generator(),
            output_signature=self.base_tensor_spec,
        )

        # if normalize do it
        if normalize:
            _tf_dataset = _tf_dataset.map(
                map_func=self.batch_transform_normalize)

        # note that our generator extracts batches from disk with certain
        # chunk_size for io efficiency ... we will first un-batch it here so
        # that we can batch it again if needed for model training
        _tf_dataset = _tf_dataset.unbatch()
        if batch_size != 1:
            _tf_dataset = _tf_dataset.batch(batch_size=batch_size)

        # if as_numpy get numpy iterator
        if as_numpy:
            _tf_dataset = _tf_dataset.as_numpy_iterator()

        # return
        return _tf_dataset

    def init_validate(self):
        """
        Any validations that can be done before dataset is initiated ...
        Useful to do separately as no need to wait until entire data is loaded.
        Happens only when __post_init__() method is called

        Returns:

        """
        # -------------------------------------------------------------- 01
        # call to super
        super().init_validate()

        # -------------------------------------------------------------- 02
        # call foster data property to do all remaining field validations
        _ = self.foster_data

        # todo: shall we do it ???
        # # -------------------------------------------------------------- 04
        # # validate batch
        # # -------------------------------------------------------------- 04.01
        # # do not validate if batch size too big
        # if self.batch_size <= 500:
        #     # ---------------------------------------------------------- 04.02
        #     # get batch by creating new iterator
        #     _first = next(iter(self.tf_dataset))
        #     _second = next(iter(self.tf_dataset))
        #     # ---------------------------------------------------------- 04.03
        #     # loop over all elements and check if equal or not equal
        #     for k in _first.keys():
        #         # val for corresponding k
        #         _first_val = _first[k].numpy()
        #         _second_val = _second[k].numpy()
        #         _is_equal = np.array_equal(_first_val, _second_val)
        #         # if singular element must be equal
        #         if len(_first_val) == 1:
        #             if len(_second_val) != 1:
        #                 e.code.CodingError(
        #                     msgs=[
        #                         f"We expect element {k} yielded by dataset "
        #                         f"to be singular",
        #                         f"Check fostered data class "
        #                         f"{self.foster_data.__class__}"
        #                     ]
        #                 )
        #             if not _is_equal:
        #                 e.code.CodingError(
        #                     msgs=[
        #                         f"We were expecting singular element {k} "
        #                         f"yielded by dataset to be always same",
        #                         f"Check fostered data class "
        #                         f"{self.foster_data.__class__}"
        #                     ]
        #                 )
        #         # else if non singular elements
        #         else:
        #             # if deterministic the must be equal
        #             if self.is_deterministic:
        #                 if not _is_equal:
        #                     e.code.CodingError(
        #                         msgs=[
        #                             f"For deterministic foster data we "
        #                             f"expect batches for element {k} ",
        #                             f"to be same.",
        #                             f"Check fostered data class "
        #                             f"{self.foster_data.__class__}"
        #                         ]
        #                     )
        #             # else must not be equal
        #             else:
        #                 if _is_equal:
        #                     e.code.CodingError(
        #                         msgs=[
        #                             f"For non deterministic foster data we "
        #                             f"expect batches for element `{k}` to be "
        #                             f"not same.",
        #                             f"Check fostered data class "
        #                             f"{self.foster_data.__class__}"
        #                         ]
        #                     )
        #     # ---------------------------------------------------------- 04.04
        #     # validate tensor spec
        #     for k, exp_ts in self.foster_data.batch_tensor_spec.items():
        #         if exp_ts.shape != _first[k].shape:
        #             e.code.CodingError(
        #                 msgs=[
        #                     f"The tensor shape for element {k} of yielded "
        #                     f"batch is not as expected",
        #                     {
        #                         "expected": exp_ts.shape,
        #                         "found": _first[k].shape,
        #                     }
        #                 ]
        #             )
        #         if exp_ts.shape[0] not in [1, self.batch_size]:
        #             e.code.CodingError(
        #                 msgs=[
        #                     f"Element {k} of yielded batch must be singular "
        #                     f"i.e 1 or have length same as batch_size i.e. "
        #                     f"{self.batch_size}",
        #                     f"Found incompatible length {exp_ts.shape[0]}, "
        #                     f"please redefine tensor_spec to have first "
        #                     f"dimension to be 1 or batch_size"
        #                 ]
        #             )
        #         if exp_ts.shape.ndims <= 1:
        #             e.code.CodingError(
        #                 msgs=[
        #                     f"We expect all elements of yielded batch "
        #                     f"to be 2 dimension or more",
        #                     f"Check element {k} and define its tensor_spec "
        #                     f"to be minimum 2D."
        #                 ]
        #             )
        #         if exp_ts.dtype != _first[k].dtype:
        #             e.code.CodingError(
        #                 msgs=[
        #                     f"The tensor dtype for element {k} of yielded "
        #                     f"batch is not as expected",
        #                     {
        #                         "expected": exp_ts.dtype,
        #                         "found": _first[k].dtype,
        #                     }
        #                 ]
        #             )

    def init(self):

        # call super
        super().init()

        # set dataset for foster data
        self.foster_data.internal.dataset = self

    # noinspection PyMethodMayBeStatic,PyUnusedLocal
    def batch_transform_normalize(
        self,
        input_dict: t.Dict[str, NPY_OR_TENSOR],
    ) -> t.Dict[str, NPY_OR_TENSOR]:
        """
        todo: Does getting in a tuple/dict and sending back a new tuple/dict
            incur overhead, or memory copy ??? Please investigate or else use
            dictionary of tensors.
        """
        e.code.NotImplementedCorrectly(
            msgs=[
                f"Please override this method if you have any batch "
                f"transformations to do ..."
            ]
        )
        return {}

    def _get_appropriate_dataset(
        self,
        dataset_class: t.Type["Dataset"] = None,
        kwargs_that_can_be_same: t.List[str] = None,
        **kwargs
    ) -> "Dataset":
        """
        Common code shared between get_appropriate_test_dataset and
        get_appropriate_validate_dataset
        """

        # -----------------------------------------------------------------01
        # kwargs cannot be empty as minimum one field need to be there to
        # distinguish between fit and other dataset
        if len(kwargs) == 0:
            e.code.CodingError(
                msgs=[
                    f"Make sure that you have overridden appropriate datasets "
                    f"methods carefully in class {self.__class__}",
                    f"There needs to be some kwargs defined in method "
                    f"kwargs that distinguish between fit and other "
                    f"dataset",
                ]
            )
        if kwargs_that_can_be_same is not None:
            if len(kwargs) == len(kwargs_that_can_be_same):
                e.code.CodingError(
                    msgs=[
                        f"You are supplying unique kwargs but all of them are "
                        f"declared are allowed to have same value ... so make "
                        f"sure that you supply more kwargs apart from "
                        f"{kwargs_that_can_be_same} so that unique dataset is "
                        f"generated ..."
                    ]
                )

        # -----------------------------------------------------------------02
        # keys in kwargs dict need to fields of dataclass
        _fs = self.dataclass_field_names
        for k in kwargs.keys():
            if k not in _fs:
                e.code.CodingError(
                    msgs=[
                        f"The kwarg `{k}` supplied to method "
                        f"{self.get_appropriate_validate_dataset} should be "
                        f"one of dataclass field of class {self.__class__}"
                    ]
                )

        # -----------------------------------------------------------------03
        # invalid kwargs_that_can_be_same if they do not appear in kwargs
        if bool(kwargs_that_can_be_same):
            for kw in kwargs_that_can_be_same:
                if kw not in kwargs.keys():
                    e.code.CodingError(
                        msgs=[
                            f"The list `kwargs_that_can_be_same` is not "
                            f"appropriate or you did not supply kwarg `{kw}`.",
                            f"The key `{kw}` is not supplied in `kwargs`."
                        ]
                    )

        # -----------------------------------------------------------------04
        # the kwarg value should be different than that of self
        for k, v in kwargs.items():
            if v == getattr(self, k):
                # it is okay for this kwarg to be same
                if bool(kwargs_that_can_be_same):
                    if k in kwargs_that_can_be_same:
                        continue
                e.validation.NotAllowed(
                    msgs=[
                        f"The value supplied for kwarg `{k}` should be not "
                        f"same to fit dataset",
                        f"We expect you to choose different value."
                    ]
                )

        # -----------------------------------------------------------------04
        # get fit dataset kwargs except for those who differ
        _fit_dataset_kwargs = {
            f_name: getattr(self, f_name)
            for f_name in self.dataclass_field_names
            if f_name not in kwargs.keys()
        }

        # -----------------------------------------------------------------05
        # bake new dataset
        # noinspection PyArgumentList
        if dataset_class is None:
            dataset_class = self.__class__
        # noinspection PyArgumentList
        _new_dataset = dataset_class(
            **{
                **_fit_dataset_kwargs,
                **kwargs
            }.copy(),
        )

        # -----------------------------------------------------------------06
        # redundant check for extra safety .... the hex_hash should not match
        if self == _new_dataset:
            e.code.ShouldNeverHappen(
                msgs=[
                    f"Looks like the new dataset is same as self ... we have "
                    f"put enough checks to make sure that this does not "
                    f"happen ... please check ..."
                ]
            )

        # -----------------------------------------------------------------06
        # return
        return _new_dataset

    def get_appropriate_test_dataset(
        self,
        dataset_class: t.Type["Dataset"] = None,
        kwargs_that_can_be_same: t.List[str] = None,
        **kwargs
    ) -> "Dataset":
        # -----------------------------------------------------------------01
        # bake test dataset
        # noinspection PyArgumentList
        _test_dataset = self._get_appropriate_dataset(
            dataset_class=dataset_class,
            kwargs_that_can_be_same=kwargs_that_can_be_same,
            **kwargs
        )

        # -----------------------------------------------------------------02
        # todo: we force for now to have test dataset from different
        #  prepared_data ... this make a lot of sense ... and may be we want
        #  to retain this behaviour
        if self.prepared_data == _test_dataset.prepared_data:
            # if there is error delete foster data files from disk
            _test_dataset.foster_data.delete(force=True)
            e.validation.NotAllowed(
                msgs=[
                    f"We expect you to use different prepared data for "
                    f"training and test ..."
                ]
            )

        # -----------------------------------------------------------------03
        # return
        return _test_dataset

    def get_appropriate_validate_dataset(
        self,
        kwargs_that_can_be_same: t.List[str] = None,
        **kwargs
    ) -> "Dataset":

        # -----------------------------------------------------------------01
        # bake validate dataset
        # noinspection PyArgumentList
        _validate_dataset = self._get_appropriate_dataset(
            kwargs_that_can_be_same=kwargs_that_can_be_same,
            **kwargs
        )

        # -----------------------------------------------------------------02
        # restrict validate dataset to be minimum _frac of the fit dataset
        _validate_examples = _validate_dataset.num_examples
        _fit_examples = self.num_examples
        _frac = 0.2
        _fit_examples_min = int(_fit_examples * _frac)
        if _validate_examples < _fit_examples_min:
            e.validation.NotAllowed(
                msgs=[
                    f"We expect validate dataset to be minimum around "
                    f"{_frac * 100}% of "
                    f"the fit dataset.",
                    {
                        "_fit_examples": _fit_examples,
                        "_fit_examples_min": _fit_examples_min,
                        "_validate_examples": _validate_examples,
                    }
                ]
            )

        # -----------------------------------------------------------------03
        # todo: we force for now to have validate dataset from same
        #  prepared_data ... this make a lot of sense ... and may be we want
        #  to retain this behaviour
        if self.prepared_data != _validate_dataset.prepared_data:
            # if there is error delete foster data files from disk
            _validate_dataset.foster_data.delete(force=True)
            e.validation.NotAllowed(
                msgs=[
                    f"We expect you to use same prepared data for both "
                    f"training and validation ..."
                ]
            )

        # -----------------------------------------------------------------04
        # return
        return _validate_dataset

    @abc.abstractmethod
    def batch_2_embeddings(
        self,
        batch: t.Dict[str, np.ndarray]
    ) -> t.Dict[str, t.Union[t.List, np.ndarray]]:
        """
        We expect a pa.Table with keys 'data' and 'label'
        """
        ...

    # noinspection PyUnusedLocal
    @s.StoreField(yields=True)
    def storage_data_for_embedding(self, mode: s.MODE_TYPE) -> pa.Table:
        # check if enough examples
        if self.LITERAL.points_for_embeddings > self.num_examples:
            e.code.CodingError(
                msgs=[
                    f"Number of examples in dataset are too less to generate "
                    f"embeddings ... should be more than "
                    f"{self.LITERAL.points_for_embeddings} but we have number "
                    f"of examples {self.num_examples}"
                ]
            )
        with logger.Spinner(
            title="Storing 2D embeddings",
            logger=_LOGGER,
        ) as spinner:
            _batch_size = 32
            _batches = self.LITERAL.points_for_embeddings//_batch_size+1
            for i, element in enumerate(
                    self(batch_size=_batch_size, as_numpy=True)):
                if i == _batches:
                    return
                spinner.text = f"extracting batch {i+1}/{_batches}"
                yield pa.table(
                    self.batch_2_embeddings(
                        batch=element
                    )
                )

    @s.StoreField()
    def storage_pca_embeddings(self, mode: s.MODE_TYPE) -> pa.Table:

        # perform
        with logger.Spinner(
            title="Compute and store 2D PCA embeddings",
            logger=_LOGGER,
        ):

            # data for embeddings
            _data_for_embeddings = self.storage_data_for_embedding(mode=mode)

            # compute pca embeddings
            _pca_embeddings = algo.compute_pca_2_components(
                data_for_embedding=_data_for_embeddings
            )

            # return embeddings
            return _pca_embeddings

    def plot_pca_embeddings(self, show: bool = False):

        # -------------------------------------------------------------------
        # get embeddings
        _embeddings = self.storage_pca_embeddings(mode='rw')

        # -------------------------------------------------------------------
        # make plot
        fig = gui.Plot(
            # todo: labels need to be unique file this bug
            #   even after things are unique it does not work :(
            # this does not work as label is not unique
            # label=f"2D Embeddings: {self.group_by_name}",
            # label is unique but still does not work
            # label=f"2D Embeddings: {self.group_by_name} {self.hex_hash}",
            # The below label works
            label=f"2D Embeddings: {self.hex_hash}",
            x_axis_name="PCA component 1", y_axis_name="PCA component 2",
            height=400,
            # width=600,
        )
        fig.add_items(
            items=gui.ScatterSeries.generate_from_npy(
                label_formatter=self.label_formatter_for_plotting,
                data_x=util.pa_to_np(_embeddings["PCA component 1"]),
                data_y=util.pa_to_np(_embeddings["PCA component 2"]),
                label=util.pa_to_np(_embeddings["label"]),
                size=1.0,
            )
        )

        # -------------------------------------------------------------------
        # return or plot
        if show:
            fig.preview()
        else:
            return fig
